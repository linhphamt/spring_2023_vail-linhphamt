# Optimization

## Topics covered in today's module
* Optimization
* Gradident Descent
* Optimizers(SGD, ADAM, etc.)

## Main takeaways from doing today's assignment
<To be filled>

## Challenging, interesting, or exciting aspects of today's assignment
I think I have some questions about question 4. I can't seem to run SGD with learning rate decay and Momentum with learning rate decay so I have to comment them out eventually in order to run the other optimization methods. Also, when compiling the model for AdaGrad optimizer, I wonder if I am supposed to change all parameters (initial_accumulator_value and epsilon) - because my code didn't seem to be running otherwise.

## Additional resources used 
<To be filled>
